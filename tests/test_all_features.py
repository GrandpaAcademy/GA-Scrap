"""
Quick test to verify all major Playwright features are working
"""

import asyncio
from ga_scrap import GAScrap


async def test_basic_features():
    """Test basic features quickly"""
    
    scraper = GAScrap(headless=True)  # Headless for faster testing
    
    try:
        await scraper.start()
        print("âœ… Browser started successfully")
        
        # Basic navigation
        await scraper.goto("https://httpbin.org/html")
        print("âœ… Navigation works")
        
        # Screenshot
        screenshot = await scraper.screenshot("test_screenshot.png")
        print(f"âœ… Screenshot: {screenshot}")
        
        # Text extraction
        text = await scraper.get_text("h1")
        print(f"âœ… Text extraction: {text}")
        
        # Network monitoring
        print(f"âœ… Network requests captured: {len(scraper.requests)}")
        print(f"âœ… Network responses captured: {len(scraper.responses)}")
        
        # Performance metrics
        metrics = await scraper.save_performance_metrics()
        print(f"âœ… Performance metrics: {len(metrics)} entries")
        
        # Cookie management
        await scraper.add_cookies([{"name": "test", "value": "value", "domain": "httpbin.org", "path": "/"}])
        cookies = await scraper.get_cookies()
        print(f"âœ… Cookie management: {len(cookies)} cookies")
        
        # JavaScript execution
        result = await scraper.execute_script("() => document.title")
        print(f"âœ… JavaScript execution: {result}")
        
        # Multiple pages
        page2 = await scraper.new_page()
        await scraper.goto("https://httpbin.org/json", page=page2)
        print(f"âœ… Multiple pages: {len(scraper.pages)} pages")
        
        # Locators
        locator = scraper.get_locator("h1")
        count = await locator.count()
        print(f"âœ… Locators: {count} elements found")
        
        # Storage state
        storage = await scraper.save_storage_state()
        print(f"âœ… Storage state: {len(storage)} entries")
        
        # Accessibility
        accessibility = await scraper.get_accessibility_tree()
        print(f"âœ… Accessibility tree captured")
        
        # Advanced waiting
        await scraper.wait_for_network_idle(timeout=5000)
        print("âœ… Network idle waiting")
        
        print("\nğŸ‰ All major features working correctly!")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        
    finally:
        await scraper.stop()


if __name__ == "__main__":
    asyncio.run(test_basic_features())
